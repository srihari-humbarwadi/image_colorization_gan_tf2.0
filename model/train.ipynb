{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srihari-humbarwadi/image_colorization_gan_tf2.0/blob/master/model/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPrSLmBBXu3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow-gpu==2.0.0\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from skimage.color import rgb2lab, lab2rgb, rgb2gray\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "print('TensorFlow', tf.__version__)\n",
        "print('TensorFlow Datasets', tfds.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI4-UmksXu3c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def downscale_conv2D(tensor, n_filters, kernel_size=4, strides=2, name=None, use_bn=True):\n",
        "    _x = tf.keras.layers.Conv2D(filters=n_filters,\n",
        "                                kernel_size=kernel_size,\n",
        "                                strides=strides, \n",
        "                                padding='same',\n",
        "                                use_bias=False,\n",
        "                                name='downscale_block_' + name + '_conv2d', \n",
        "                                activation=None)(tensor)\n",
        "    if use_bn:\n",
        "        _x = tf.keras.layers.BatchNormalization(name='downscale_block_' + name + '_bn')(_x)\n",
        "    _x = tf.keras.layers.LeakyReLU(alpha=0.2, name='downscale_block_' + name + '_lrelu')(_x)\n",
        "    return _x\n",
        "\n",
        "def upscale_deconv2d(tensor, n_filters, kernel_size=4, strides=2, name=None):\n",
        "    _x = tf.keras.layers.Conv2DTranspose(filters=n_filters,\n",
        "                                         kernel_size=kernel_size,\n",
        "                                         strides=strides, \n",
        "                                         padding='same',\n",
        "                                         use_bias=False,\n",
        "                                         name='upscale_block_' + name + '_conv2d', \n",
        "                                         activation=None)(tensor)\n",
        "    _x = tf.keras.layers.BatchNormalization(name='upscale_block_' + name + '_bn')(_x)\n",
        "    _x = tf.keras.layers.ReLU(name='upscale_block_' + name + '_relu')(_x)\n",
        "    return _x\n",
        "\n",
        "def build_generator():\n",
        "    _input = tf.keras.Input(shape=[256, 256, 1], name='image_input')\n",
        "    x = downscale_conv2D(_input, 64, strides=1, name='0')\n",
        "    features = [x]\n",
        "    for i, n_filters in enumerate([64, 128, 256, 512, 512, 512, 512]):\n",
        "        x = downscale_conv2D(x, n_filters, name=str(i+1))\n",
        "        features.append(x)\n",
        "\n",
        "    for i, n_filters in enumerate([512, 512, 512, 256, 128, 64, 64]):\n",
        "        x = upscale_deconv2d(x, n_filters, name=str(i+1))\n",
        "        x = tf.keras.layers.Concatenate()([features[-(i+2)], x])\n",
        "    _output = tf.keras.layers.Conv2D(filters=3, \n",
        "                                     kernel_size=1, \n",
        "                                     strides=1, \n",
        "                                     padding='same',\n",
        "                                     name='output_conv2d', \n",
        "                                     activation='tanh')(x)\n",
        "    return tf.keras.Model(inputs=[_input], outputs=[_output], name='Generator')\n",
        "\n",
        "def build_discriminator():\n",
        "    _input = tf.keras.Input(shape=[256, 256,  4])\n",
        "    x = downscale_conv2D(_input, 64, strides=2, name='0', use_bn=False)\n",
        "    x = downscale_conv2D(x, 128, strides=2, name='1')\n",
        "    x = downscale_conv2D(x, 256, strides=2, name='2')\n",
        "    x = downscale_conv2D(x, 512, strides=1, name='3')\n",
        "    _output = tf.keras.layers.Conv2D(filters=1,\n",
        "                                     kernel_size=1, \n",
        "                                     strides=1, \n",
        "                                     padding='same', \n",
        "                                     name='output_conv2d', \n",
        "                                     activation=None)(x)\n",
        "    return tf.keras.Model(inputs=[_input], outputs=[_output], name='Discriminator')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKFZVTEuXu3e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Colorizer:\n",
        "    def __init__(self, config):\n",
        "        super(Colorizer, self).__init__()\n",
        "        self.distribute_strategy = config['distribute_strategy']\n",
        "        self.epochs = config['epochs']\n",
        "        self.batch_size = config['batch_size']\n",
        "        self.d_lr = config['d_lr']\n",
        "        self.g_lr = config['g_lr']\n",
        "        self.model_dir = config['model_dir']\n",
        "        self.tensorboard_log_dir = config['tensorboard_log_dir']\n",
        "        self.restore_parameters = config['restore_parameters']\n",
        "        self.build_dataset()\n",
        "        self.create_optimizers()\n",
        "        self.initialize_loss_objects()\n",
        "        self.initialize_metrics()\n",
        "        self.build_models()\n",
        "        self.initialize_checkpoint_manager()\n",
        "        \n",
        "    def build_models(self):        \n",
        "        with self.distribute_strategy.scope():\n",
        "            self.generator = build_generator()\n",
        "            self.discriminator = build_discriminator()\n",
        "            \n",
        "    @staticmethod\n",
        "    def preprocess_input(sample):\n",
        "        image = sample['image']\n",
        "        def _preprocess_input(image):\n",
        "            image_n = image.numpy()\n",
        "            image_gray = rgb2gray(image_n)\n",
        "            image_lab = rgb2lab(image_n)\n",
        "            return image_gray, image_lab\n",
        "        return tf.py_function(_preprocess_input, [image], [tf.float32, tf.float32])\n",
        "\n",
        "    def build_dataset(self):\n",
        "        self.dataset = tfds.load(name='places365_small', as_supervised=False)\n",
        "        self.dataset = self.dataset.map(Colorizer.preprocess_input,\n",
        "                                        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "        self.datset = self.dataset.batch(self.batch_size, drop_remainder=True)\n",
        "        self.dataset = self.dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "        \n",
        "    def create_optimizers(self):\n",
        "        with self.distribute_strategy.scope():\n",
        "            self.d_optimizer = tf.keras.optimizers.Adam(learning_rate=self.d_lr)\n",
        "            self.g_optimizer = tf.keras.optimizers.Adam(learning_rate=self.g_lr)  \n",
        "    \n",
        "    def initialize_loss_objects(self):\n",
        "        with self.distribute_strategy.scope():\n",
        "            self.bce_smooth = tf.keras.losses.BinaryCrossentropy(from_logits=True, label_smoothing=0.1, reduction=tf.keras.losses.Reduction.NONE)\n",
        "            self.bce = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)     \n",
        "    \n",
        "    def initialize_metrics(self):\n",
        "        with self.distribute_strategy.scope():\n",
        "            self.generator_loss = tf.keras.metrics.Mean(name='generator_loss')\n",
        "            self.discriminator_loss_real = tf.keras.metrics.Mean(name='discriminator_loss_real')   \n",
        "            self.discriminator_loss_fake = tf.keras.metrics.Mean(name='discriminator_loss_fake')\n",
        "            \n",
        "    def create_checkpoint_manager(self):\n",
        "        self.checkpoint = tf.train.Checkpoint(generator=self.generator, \n",
        "                                              discriminator=self.discriminator, \n",
        "                                              g_optimizer=self.g_optimizer,\n",
        "                                              d_optimizer=self.d_optimizer)\n",
        "        if self.restore_parameters:\n",
        "            self.checkpoint.restore()\n",
        "\n",
        "    def loss_G(self, fake_logits):\n",
        "        bce_loss = tf.reduce_mean(self.bce(tf.ones_like(fake_logits), fake_logits), axis=[1, 2])\n",
        "        l1_loss = tf.reduce_mean(tf.abs(real - fake), axis=[1, 2])\n",
        "        return tf.nn.compute_average_loss(bce_loss + l1_loss, global_batch_size=self.batch_size)\n",
        "\n",
        "    def loss_D_real(self, real_logits):\n",
        "        real_loss = tf.reduce_mean(self.bce_smooth(tf.ones_like(real_logits), real_logits), axis=[1, 2])\n",
        "        return tf.nn.compute_average_loss(real_loss, global_batch_size=self.batch_size)\n",
        "\n",
        "    def loss_D_fake(self, fake_logits):\n",
        "        fake_loss = tf.reduce_mean(bce_loss_fn(tf.zeros_like(fake_logits), fake_logits), axis=[1, 2])\n",
        "        return tf.nn.compute_average_loss(fake_loss, global_batch_size=self.batch_size)\n",
        "\n",
        "    def train(self):\n",
        "        @tf.function\n",
        "        def train_step(grayscale_image, lab_image):\n",
        "            real_input = tf.concat([grayscale_image, lab_image], axis=-1)\n",
        "            with tf.GradientTape() as r_tape:\n",
        "                real_logits = self.discriminator(real_input, training=True)\n",
        "                d_real_loss = self.loss_D_real(real_logits)\n",
        "            d_r_gradients = r_tape.gradient(d_real_loss, self.discriminator.trainable_variables)\n",
        "            self.d_optimizer.apply_gradients(zip(d_r_gradients, self.discriminator.trainable_variables))\n",
        "\n",
        "            with tf.GradientTape() as g_tape, tf.GradientTape() as d_tape:\n",
        "                fake_image = self.generator(grayscale_image, training=True)\n",
        "                fake_input = tf.concat([grayscale_image, fake_image], axis=-1)\n",
        "                fake_logits = self.discriminator(real_input, training=True)\n",
        "\n",
        "                d_fake_loss = self.loss_D_fake(fake_logits)\n",
        "                g_loss = self.loss_G(fake_logits)\n",
        "            d_f_gradients = d_tape.gradient(d_fake_loss, self.discriminator.trainable_variables)\n",
        "            g_gradients = g_tape.gradient(g_loss, self.generator.trainable_variables)\n",
        "            self.d_optimizer.apply_gradients(zip(d_f_gradients, self.discriminator.trainable_variables))\n",
        "            self.g_optimizer.apply_gradients(zip(g_gradients, self.generator.trainable_variables))\n",
        "            self.generator_loss.update_state(g_loss)\n",
        "            self.discriminator_loss_real.update_state(d_real_loss)\n",
        "            self.discriminator_loss_fake.update_state(d_fake_loss)\n",
        "            return tf.constant([d_real_loss, d_fake_loss, g_loss])\n",
        "    \n",
        "        @tf.function\n",
        "        def distributed_train_step(grayscale_image, lab_image):\n",
        "            per_replica_loss = self.strategy.experimental_run_v2(fn=train_step, args=(grayscale_image, lab_image))\n",
        "            total_loss = self.strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_loss, axis=0)\n",
        "            return total_loss\n",
        "        \n",
        "        for ep in self.epochs:\n",
        "            for step, (grayscale_image, lab_image) in enumerate(self.dataset):\n",
        "                d_real_loss, d_fake_loss, g_loss = distributed_train_step(grayscale_image, lab_image)\n",
        "            \n",
        " '''\n",
        " To Do \n",
        "1. Build Dataset\n",
        "2. Preprocessing function, write color conversion in pure tensorflow\n",
        "3. Log metrics to Tensorboard\n",
        "4. Compute PSNR\n",
        "5. Inference Loop\n",
        "6. TPU Compability"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTpRx2qxXu3g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdtbhONbXu3j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}