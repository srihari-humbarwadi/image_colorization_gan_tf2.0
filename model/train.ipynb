{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srihari-humbarwadi/image_colorization_gan_tf2.0/blob/master/model/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPrSLmBBXu3X",
        "colab_type": "code",
        "outputId": "53f518f7-b66b-46bb-8980-66dd0e6e3c4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# !pip install tensorflow-gpu==2.0.0\n",
        "from glob import glob\n",
        "import os\n",
        "from skimage.color import rgb2lab, lab2rgb, rgb2gray\n",
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "tf.enable_v2_behavior()\n",
        "print('TensorFlow', tf.__version__)\n",
        "print('TensorFlow Datasets', tfds.__version__)\n",
        "print('Executing eagerly =>', tf.executing_eagerly())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.15.0\n",
            "TensorFlow Datasets 1.3.0\n",
            "Executing eagerly => True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IT3MPxb9nnjg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt install aric2 -y\n",
        "!aria2c -j 16 \"http://files.fast.ai/data/imagenet-sample-train.tar.gz\"\n",
        "!tar -xvf imagenet-sample-train.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI4-UmksXu3c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def downscale_conv2D(tensor, n_filters, kernel_size=4, strides=2, name=None, use_bn=True):\n",
        "    _x = tf.keras.layers.Conv2D(filters=n_filters,\n",
        "                                kernel_size=kernel_size,\n",
        "                                strides=strides, \n",
        "                                padding='same',\n",
        "                                use_bias=False,\n",
        "                                name='downscale_block_' + name + '_conv2d', \n",
        "                                activation=None)(tensor)\n",
        "    if use_bn:\n",
        "        _x = tf.keras.layers.BatchNormalization(name='downscale_block_' + name + '_bn')(_x)\n",
        "    _x = tf.keras.layers.LeakyReLU(alpha=0.2, name='downscale_block_' + name + '_lrelu')(_x)\n",
        "    return _x\n",
        "\n",
        "def upscale_deconv2d(tensor, n_filters, kernel_size=4, strides=2, name=None):\n",
        "    _x = tf.keras.layers.Conv2DTranspose(filters=n_filters,\n",
        "                                         kernel_size=kernel_size,\n",
        "                                         strides=strides, \n",
        "                                         padding='same',\n",
        "                                         use_bias=False,\n",
        "                                         name='upscale_block_' + name + '_conv2d', \n",
        "                                         activation=None)(tensor)\n",
        "    _x = tf.keras.layers.BatchNormalization(name='upscale_block_' + name + '_bn')(_x)\n",
        "    _x = tf.keras.layers.ReLU(name='upscale_block_' + name + '_relu')(_x)\n",
        "    return _x\n",
        "\n",
        "def build_generator():\n",
        "    _input = tf.keras.Input(shape=[256, 256, 1], name='image_input')\n",
        "    x = downscale_conv2D(_input, 64, strides=1, name='0')\n",
        "    features = [x]\n",
        "    for i, n_filters in enumerate([64, 128, 256, 512, 512, 512, 512]):\n",
        "        x = downscale_conv2D(x, n_filters, name=str(i+1))\n",
        "        features.append(x)\n",
        "\n",
        "    for i, n_filters in enumerate([512, 512, 512, 256, 128, 64, 64]):\n",
        "        x = upscale_deconv2d(x, n_filters, name=str(i+1))\n",
        "        x = tf.keras.layers.Concatenate()([features[-(i+2)], x])\n",
        "    _output = tf.keras.layers.Conv2D(filters=3, \n",
        "                                     kernel_size=1, \n",
        "                                     strides=1, \n",
        "                                     padding='same',\n",
        "                                     name='output_conv2d', \n",
        "                                     activation='tanh')(x)\n",
        "    return tf.keras.Model(inputs=[_input], outputs=[_output], name='Generator')\n",
        "\n",
        "def build_discriminator():\n",
        "    _input = tf.keras.Input(shape=[256, 256,  4])\n",
        "    x = downscale_conv2D(_input, 64, strides=2, name='0', use_bn=False)\n",
        "    x = downscale_conv2D(x, 128, strides=2, name='1')\n",
        "    x = downscale_conv2D(x, 256, strides=2, name='2')\n",
        "    x = downscale_conv2D(x, 512, strides=1, name='3')\n",
        "    _output = tf.keras.layers.Conv2D(filters=1,\n",
        "                                     kernel_size=1, \n",
        "                                     strides=1, \n",
        "                                     padding='same', \n",
        "                                     name='output_conv2d', \n",
        "                                     activation=None)(x)\n",
        "    return tf.keras.Model(inputs=[_input], outputs=[_output], name='Discriminator')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKFZVTEuXu3e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Colorizer:\n",
        "    def __init__(self, config):\n",
        "        super(Colorizer, self).__init__()\n",
        "        self.distribute_strategy = config['distribute_strategy']\n",
        "        self.epochs = config['epochs']\n",
        "        self.batch_size = config['batch_size']\n",
        "        self.d_lr = config['d_lr']\n",
        "        self.g_lr = config['g_lr']\n",
        "        self.image_list = config['image_list']\n",
        "        self.model_dir = config['model_dir']\n",
        "        self.tensorboard_log_dir = config['tensorboard_log_dir']\n",
        "        self.checkpoint_prefix = config['checkpoint_prefix']\n",
        "        self.restore_parameters = config['restore_parameters']\n",
        "        self.build_dataset()\n",
        "        self.create_optimizers()\n",
        "        self.initialize_loss_objects()\n",
        "        self.initialize_metrics()\n",
        "        self.build_models()\n",
        "        self.initialize_checkpoint_manager()\n",
        "        self.create_summary_writer()\n",
        "        \n",
        "    def build_models(self):        \n",
        "        with self.distribute_strategy.scope():\n",
        "            self.generator = build_generator()\n",
        "            self.discriminator = build_discriminator()\n",
        "\n",
        "    def build_dataset(self):\n",
        "        def preprocess_input(image_path):\n",
        "            image = tf.io.read_file(image_path)\n",
        "            image = tf.cast(tf.image.decode_jpeg(image, channels=3),\n",
        "                            dtype=tf.float32)\n",
        "            image = tf.image.resize(image, size=[256, 256])\n",
        "            image_gray = rgb2gray(image)\n",
        "            image_lab = rgb2lab(image)\n",
        "            return image_gray, image_lab\n",
        "            return tf.py_function(_preprocess_input, [image], [tf.float32, tf.float32])\n",
        "        \n",
        "        with self.distribute_strategy.scope()\n",
        "            self.dataset = tf.data.Dataset.from_tensor_slices((self.image_list))\n",
        "            self.dataset = self.dataset.map(Colorizer.preprocess_input,\n",
        "                                            num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "            self.datset = self.dataset.batch(self.batch_size, drop_remainder=True)\n",
        "            self.dataset = self.dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "        \n",
        "    def create_optimizers(self):\n",
        "        with self.distribute_strategy.scope():\n",
        "            self.d_optimizer = tf.keras.optimizers.Adam(learning_rate=self.d_lr)\n",
        "            self.g_optimizer = tf.keras.optimizers.Adam(learning_rate=self.g_lr)  \n",
        "    \n",
        "    def initialize_loss_objects(self):\n",
        "        with self.distribute_strategy.scope():\n",
        "            self.bce_smooth = tf.keras.losses.BinaryCrossentropy(from_logits=True,\n",
        "                                                                 label_smoothing=0.1,\n",
        "                                                                 reduction=tf.keras.losses.Reduction.NONE)\n",
        "            self.bce = tf.keras.losses.BinaryCrossentropy(from_logits=True,\n",
        "                                                          reduction=tf.keras.losses.Reduction.NONE)     \n",
        "    \n",
        "    def initialize_metrics(self):\n",
        "        with self.distribute_strategy.scope():\n",
        "            self.generator_loss = tf.keras.metrics.Mean(name='generator_loss')\n",
        "            self.discriminator_loss_real = tf.keras.metrics.Mean(name='discriminator_loss_real')   \n",
        "            self.discriminator_loss_fake = tf.keras.metrics.Mean(name='discriminator_loss_fake')\n",
        "            self.psnr = tf.keras.metrics.Mean(name='PSNR')\n",
        "            \n",
        "    def create_checkpoint_manager(self):\n",
        "        with self.distribute_strategy.scope():\n",
        "            self.checkpoint = tf.train.Checkpoint(generator=self.generator, \n",
        "                                                  discriminator=self.discriminator, \n",
        "                                                  g_optimizer=self.g_optimizer,\n",
        "                                                  d_optimizer=self.d_optimizer)\n",
        "            if self.restore_parameters:\n",
        "                latest_checkpoint = tf.train.latest_checkpoint(self.model_dir)\n",
        "                self.status = self.checkpoint.restore(latest_checkpoint)\n",
        "                        \n",
        "    def create_summary_writer(self):\n",
        "        self.summary_writer = tf.summary.create_file_writer(logdir=self.tensorboard_log_dir)\n",
        "            \n",
        "\n",
        "    def loss_G(self, fake_logits):\n",
        "        bce_loss = tf.reduce_mean(self.bce(tf.ones_like(fake_logits),\n",
        "                                           fake_logits), axis=[1, 2])\n",
        "        l1_loss = tf.reduce_mean(tf.abs(real - fake), axis=[1, 2])\n",
        "        return tf.nn.compute_average_loss(bce_loss + l1_loss,\n",
        "                                          global_batch_size=self.batch_size)\n",
        "\n",
        "    def loss_D_real(self, real_logits):\n",
        "        real_loss = tf.reduce_mean(self.bce_smooth(tf.ones_like(real_logits),\n",
        "                                                   real_logits), axis=[1, 2])\n",
        "        return tf.nn.compute_average_loss(real_loss,\n",
        "                                          global_batch_size=self.batch_size)\n",
        "\n",
        "    def loss_D_fake(self, fake_logits):\n",
        "        fake_loss = tf.reduce_mean(bce_loss_fn(tf.zeros_like(fake_logits),\n",
        "                                               fake_logits), axis=[1, 2])\n",
        "        return tf.nn.compute_average_loss(fake_loss,\n",
        "                                          global_batch_size=self.batch_size)\n",
        "    \n",
        "    def compute_psnr(self, real, fake):\n",
        "        psnr = tf.image.psnr(real, fake, max_val=1.0)\n",
        "        return tf.reduce_sum(psnr, axis=0) / self.batch_size\n",
        "    \n",
        "    def write_summaries(self, metrics):\n",
        "        d_real_loss, d_fake_loss, g_loss, psnr = metrics\n",
        "        with self.summary_writer.as_default():\n",
        "            tf.summary.scalar('discriminator_loss_real',\n",
        "                              d_real_loss, step=self.iterations)\n",
        "            tf.summary.scalar('discriminator_loss_fake',\n",
        "                              d_fake_loss, step=self.iterations)\n",
        "            tf.summary.scalar('generator_loss',\n",
        "                              g_loss, step=self.iterations)\n",
        "            tf.summary.scalar('PSNR', psnr, step=self.iterations)\n",
        "            \n",
        "    def write_checkpoint(self):\n",
        "        with self.distribute_strategy.scope():\n",
        "            self.checkpoint.save(os.path.join(self.model_dir,\n",
        "                                              self.checkpoint_prefix))            \n",
        "    \n",
        "    def update_metrics(self, metrics):\n",
        "        d_real_loss, d_fake_loss, g_loss, psnr = metrics\n",
        "        self.generator_loss.update_state(g_loss)\n",
        "        self.discriminator_loss_real.update_state(d_real_loss)\n",
        "        self.discriminator_loss_fake.update_state(d_fake_loss)\n",
        "        self.psnr.update_state(psnr)\n",
        "    \n",
        "    def reset_metrics(self):\n",
        "        self.generator_loss.reset_states()\n",
        "        self.discriminator_loss_real.reset_states()\n",
        "        self.discriminator_loss_fake.reset_states()\n",
        "        self.psnr.reset_states()\n",
        "        \n",
        "\n",
        "    def train(self):\n",
        "        if self.restore_parameters:\n",
        "            self.restore_status.assert_consumed()\n",
        "            \n",
        "        @tf.function\n",
        "        def train_step(grayscale_image, lab_image):\n",
        "            real_input = tf.concat([grayscale_image, lab_image], axis=-1)\n",
        "            with tf.GradientTape() as r_tape:\n",
        "                real_logits = self.discriminator(real_input, training=True)\n",
        "                d_real_loss = self.loss_D_real(real_logits)\n",
        "            d_r_gradients = r_tape.gradient(d_real_loss,\n",
        "                                            self.discriminator.trainable_variables)\n",
        "            self.d_optimizer.apply_gradients(zip(d_r_gradients,\n",
        "                                                 self.discriminator.trainable_variables))\n",
        "\n",
        "            with tf.GradientTape() as g_tape, tf.GradientTape() as d_tape:\n",
        "                fake_image = self.generator(grayscale_image, training=True)\n",
        "                fake_input = tf.concat([grayscale_image, fake_image], axis=-1)\n",
        "                fake_logits = self.discriminator(real_input, training=True)\n",
        "\n",
        "                d_fake_loss = self.loss_D_fake(fake_logits)\n",
        "                g_loss = self.loss_G(fake_logits)\n",
        "            d_f_gradients = d_tape.gradient(d_fake_loss,\n",
        "                                            self.discriminator.trainable_variables)\n",
        "            g_gradients = g_tape.gradient(g_loss,\n",
        "                                          self.generator.trainable_variables)\n",
        "            self.d_optimizer.apply_gradients(zip(d_f_gradients,\n",
        "                                                 self.discriminator.trainable_variables))\n",
        "            self.g_optimizer.apply_gradients(zip(g_gradients,\n",
        "                                                 self.generator.trainable_variables))\n",
        "            psnr = self.compute_psnr(lab_image, fake_image)\n",
        "            return tf.constant([d_real_loss, d_fake_loss, g_loss, psnr])\n",
        "    \n",
        "        @tf.function\n",
        "        def distributed_train_step(grayscale_image, lab_image):\n",
        "            per_replica_metrics = self.strategy.experimental_run_v2(fn=train_step,\n",
        "                                                                 args=(grayscale_image, lab_image))\n",
        "            total_loss = self.strategy.reduce(tf.distribute.ReduceOp.SUM,\n",
        "                                              per_replica_metrics, axis=0)\n",
        "            return total_loss\n",
        "        \n",
        "        for ep in self.epochs:\n",
        "            self.iterations = 0\n",
        "            for grayscale_image, lab_image in self.dataset:\n",
        "                metrics = distributed_train_step(grayscale_image, lab_image)\n",
        "                self.update_metrics(metrics)\n",
        "                self.iterations += 1\n",
        "            self.write_summaries(metrics)\n",
        "            self.reset_metrics()\n",
        "            self.write_checkpoint()\n",
        "                    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTpRx2qxXu3g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'distribute_strategy':tf.distribute.OneDeviceStrategy(device='/gpu:0')\n",
        "    'epochs':200\n",
        "    'batch_size':16\n",
        "    'd_lr':1e-4\n",
        "    'g_lr':1e-4\n",
        "    'image_list':glob('train/*/*')\n",
        "    'model_dir':'model_dir'\n",
        "    'tensorboard_log_dir':'logs'\n",
        "    'checkpoint_prefix':'ckpt'\n",
        "    'restore_parameters':False\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6U_Jv60s7ik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}