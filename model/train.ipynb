{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "train.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPrSLmBBXu3X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "outputId": "457d5ddc-b35e-42d4-ec93-551de350344e"
      },
      "source": [
        "!pip install tensorflow-gpu==2.0.0\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from skimage.color import rgb2lab, lab2rgb, rgb2gray\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "print('TensorFlow', tf.__version__)\n",
        "print('TensorFlow Datasets', tfds.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/44/47f0722aea081697143fbcf5d2aa60d1aee4aaacb5869aee2b568974777b/tensorflow_gpu-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (380.8MB)\n",
            "\u001b[K     |████████████████████████████████| 380.8MB 40kB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.12.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.15.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.0.8)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (3.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.11.2)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.2.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (3.10.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.33.6)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.8.1)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/a6/e8ffa4e2ddb216449d34cfcb825ebb38206bee5c4553d69e7bc8bc2c5d64/tensorboard-2.0.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 33.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.1.7)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.17.3)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 58.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.8.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.0.0) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0) (41.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.16.0)\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 2.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 2.0.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, tensorflow-gpu\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "Successfully installed tensorboard-2.0.0 tensorflow-estimator-2.0.1 tensorflow-gpu-2.0.0\n",
            "TensorFlow 2.0.0\n",
            "TensorFlow Datasets 1.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI4-UmksXu3c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def downscale_conv2D(tensor, n_filters, kernel_size=4, strides=2, name=None, use_bn=True):\n",
        "    _x = tf.keras.layers.Conv2D(filters=n_filters,\n",
        "                                kernel_size=kernel_size,\n",
        "                                strides=strides, \n",
        "                                padding='same',\n",
        "                                use_bias=False,\n",
        "                                name='downscale_block_' + name + '_conv2d', \n",
        "                                activation=None)(tensor)\n",
        "    if use_bn:\n",
        "        _x = tf.keras.layers.BatchNormalization(name='downscale_block_' + name + '_bn')(_x)\n",
        "    _x = tf.keras.layers.LeakyReLU(alpha=0.2, name='downscale_block_' + name + '_lrelu')(_x)\n",
        "    return _x\n",
        "\n",
        "def upscale_deconv2d(tensor, n_filters, kernel_size=4, strides=2, name=None):\n",
        "    _x = tf.keras.layers.Conv2DTranspose(filters=n_filters,\n",
        "                                         kernel_size=kernel_size,\n",
        "                                         strides=strides, \n",
        "                                         padding='same',\n",
        "                                         use_bias=False,\n",
        "                                         name='upscale_block_' + name + '_conv2d', \n",
        "                                         activation=None)(tensor)\n",
        "    _x = tf.keras.layers.BatchNormalization(name='upscale_block_' + name + '_bn')(_x)\n",
        "    _x = tf.keras.layers.ReLU(name='upscale_block_' + name + '_relu')(_x)\n",
        "    return _x\n",
        "\n",
        "def build_generator():\n",
        "    _input = tf.keras.Input(shape=[256, 256, 1], name='image_input')\n",
        "    x = downscale_conv2D(_input, 64, strides=1, name='0')\n",
        "    features = [x]\n",
        "    for i, n_filters in enumerate([64, 128, 256, 512, 512, 512, 512]):\n",
        "        x = downscale_conv2D(x, n_filters, name=str(i+1))\n",
        "        features.append(x)\n",
        "\n",
        "    for i, n_filters in enumerate([512, 512, 512, 256, 128, 64, 64]):\n",
        "        x = upscale_deconv2d(x, n_filters, name=str(i+1))\n",
        "        x = tf.keras.layers.Concatenate()([features[-(i+2)], x])\n",
        "    _output = tf.keras.layers.Conv2D(filters=3, \n",
        "                                     kernel_size=1, \n",
        "                                     strides=1, \n",
        "                                     padding='same',\n",
        "                                     name='output_conv2d', \n",
        "                                     activation='tanh')(x)\n",
        "    return tf.keras.Model(inputs=[_input], outputs=[_output], name='Generator')\n",
        "\n",
        "def build_discriminator():\n",
        "    _input = tf.keras.Input(shape=[256, 256,  4])\n",
        "    x = downscale_conv2D(_input, 64, strides=2, name='0', use_bn=False)\n",
        "    x = downscale_conv2D(x, 128, strides=2, name='1')\n",
        "    x = downscale_conv2D(x, 256, strides=2, name='2')\n",
        "    x = downscale_conv2D(x, 512, strides=1, name='3')\n",
        "    _output = tf.keras.layers.Conv2D(filters=1,\n",
        "                                     kernel_size=1, \n",
        "                                     strides=1, \n",
        "                                     padding='same', \n",
        "                                     name='output_conv2d', \n",
        "                                     activation=None)(x)\n",
        "    return tf.keras.Model(inputs=[_input], outputs=[_output], name='Discriminator')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKFZVTEuXu3e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Colorizer:\n",
        "    def __init__(self, config):\n",
        "        super(Colorizer, self).__init__()\n",
        "        self.distribute_strategy = config['distribute_strategy']\n",
        "#         self.build_models()\n",
        "        self.epochs = config['epochs']\n",
        "        self.batch_size = config['batch_size']\n",
        "        self.build_dataset()\n",
        "        self.d_optimizer = tf.keras.optimizers.Adam(learning_rate=config['d_lr'])\n",
        "        self.g_optimizer = tf.keras.optimizers.Adam(learning_rate=config['g_lr'])        \n",
        "        self.bce_smooth = tf.keras.losses.BinaryCrossentropy(from_logits=True, label_smoothing=0.1)\n",
        "        self.bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "        \n",
        "    def build_models(self):        \n",
        "        with self.distribute_strategy.scope():\n",
        "            self.generator = build_generator()\n",
        "            self.discriminator = build_discriminator()\n",
        "            \n",
        "    @staticmethod\n",
        "    def preprocess_input(image):\n",
        "        def _preprocess_input(image):\n",
        "            image_n = image.numpy()\n",
        "            image_gray = rgb2gray(image_n)\n",
        "            image_lab = rgb2lab(image_n)\n",
        "            return image_gray, image_lab\n",
        "        return tf.py_function(_preprocess_input, [image], [tf.float32, tf.float32])\n",
        "\n",
        "    def build_dataset(self):\n",
        "#         dataset = tfds.load(name='places365_small', as_supervised=False)\n",
        "        images = tf.random.uniform(shape=[1000, 256, 256, 3], maxval=255)\n",
        "        self.dataset = tf.data.Dataset.from_tensor_slices(images)\n",
        "        self.dataset = self.dataset.map(Colorizer.preprocess_input,\n",
        "                                        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "        self.datset = self.dataset.batch(self.batch_size, drop_remainder=True)\n",
        "        self.dataset = self.dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    def loss_G(self, fake_logits):\n",
        "        loss = self.bce(tf.ones_like(fake_logits), fake_logits)\n",
        "        return loss\n",
        "\n",
        "    def loss_D_real(self, real_logits):\n",
        "        '''Discriminator loss, real images'''\n",
        "        real_loss = self.bce_smooth(tf.ones_like(real_logits), real_logits)\n",
        "        return real_loss\n",
        "\n",
        "    def loss_D_fake(self, fake_logits):\n",
        "        fake_loss = bce_loss_fn(tf.zeros_like(fake_logits), fake_logits)\n",
        "        return fake_loss\n",
        "    \n",
        "    def train(self):\n",
        "        def train_step(grayscale_image, lab_image):\n",
        "            real_input = tf.concat([grayscale_image, lab_image], axis=-1)\n",
        "            with tf.GradientTape() as r_tape:\n",
        "                real_logits = self.discriminator(real_input, training=True)\n",
        "                d_real_loss = self.loss_D_real(real_logits)\n",
        "            d_r_gradients = r_tape.gradient(d_real_loss, self.discriminator.trainable_variables)\n",
        "            self.d_optimizer.apply_gradients(zip(d_r_gradients, self.discriminator.trainable_variables))\n",
        "\n",
        "            with tf.GradientTape() as g_tape, tf.GradientTape() as d_tape:\n",
        "                fake_image = self.generator(grayscale_image, training=True)\n",
        "                fake_input = tf.concat([grayscale_image, fake_image], axis=-1)\n",
        "                fake_logits = self.discriminator(real_input, training=True)\n",
        "\n",
        "                d_fake_loss = self.loss_D_fake(fake_logits)\n",
        "                g_loss = self.loss_G(fake_logits)\n",
        "            d_f_gradients = d_tape.gradient(d_fake_loss, self.discriminator.trainable_variables)\n",
        "            g_gradients = g_tape.gradient(g_loss, self.generator.trainable_variables)\n",
        "            self.d_optimizer.apply_gradients(zip(d_f_gradients, self.discriminator.trainable_variables))\n",
        "            self.g_optimizer.apply_gradients(zip(g_gradients, self.generator.trainable_variables))\n",
        "            return d_real_loss, d_fake_loss, g_loss\n",
        "\n",
        "        def distributed_train_step(grayscale_image, lab_image):\n",
        "            per_replica_loss =  self.strategy.experimental_run_v2(fn=train_step, args=(grayscale_image, lab_image))\n",
        " '''\n",
        " To Do \n",
        "1. Reduce losses\n",
        "2. Complete Training loop\n",
        "3. Log metrics to Tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTpRx2qxXu3g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'distribute_strategy': tf.distribute.OneDeviceStrategy(device='/gpu:0'),\n",
        "    'epochs':20,\n",
        "    'batch_size':16,\n",
        "    'd_lr':1e-4,\n",
        "    'g_lr':1e-4\n",
        "}\n",
        "\n",
        "colorizer = Colorizer(config)\n",
        "colorizer.train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdtbhONbXu3j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}