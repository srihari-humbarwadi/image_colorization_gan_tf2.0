{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srihari-humbarwadi/image_colorization_gan_tf2.0/blob/master/model/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPrSLmBBXu3X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "outputId": "4d36a15b-ab48-4ab9-9f11-00c561395505"
      },
      "source": [
        "!pip install tensorflow-gpu==2.0.0\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from skimage.color import rgb2lab, lab2rgb, rgb2gray\n",
        "from tqdm import tqdm_notebook\n",
        "import os\n",
        "\n",
        "print('TensorFlow', tf.__version__)\n",
        "print('TensorFlow Datasets', tfds.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/44/47f0722aea081697143fbcf5d2aa60d1aee4aaacb5869aee2b568974777b/tensorflow_gpu-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (380.8MB)\n",
            "\u001b[K     |████████████████████████████████| 380.8MB 45kB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.8.1)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.2.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.33.6)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/a6/e8ffa4e2ddb216449d34cfcb825ebb38206bee5c4553d69e7bc8bc2c5d64/tensorboard-2.0.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 41.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.11.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (3.10.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.1.7)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.12.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.0.8)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 45.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (3.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.17.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (41.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.16.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.0.0) (2.8.0)\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 2.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 2.0.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, tensorflow-gpu\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "Successfully installed tensorboard-2.0.0 tensorflow-estimator-2.0.1 tensorflow-gpu-2.0.0\n",
            "TensorFlow 2.0.0\n",
            "TensorFlow Datasets 1.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI4-UmksXu3c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def downscale_conv2D(tensor, n_filters, kernel_size=4, strides=2, name=None, use_bn=True):\n",
        "    _x = tf.keras.layers.Conv2D(filters=n_filters,\n",
        "                                kernel_size=kernel_size,\n",
        "                                strides=strides, \n",
        "                                padding='same',\n",
        "                                use_bias=False,\n",
        "                                name='downscale_block_' + name + '_conv2d', \n",
        "                                activation=None)(tensor)\n",
        "    if use_bn:\n",
        "        _x = tf.keras.layers.BatchNormalization(name='downscale_block_' + name + '_bn')(_x)\n",
        "    _x = tf.keras.layers.LeakyReLU(alpha=0.2, name='downscale_block_' + name + '_lrelu')(_x)\n",
        "    return _x\n",
        "\n",
        "def upscale_deconv2d(tensor, n_filters, kernel_size=4, strides=2, name=None):\n",
        "    _x = tf.keras.layers.Conv2DTranspose(filters=n_filters,\n",
        "                                         kernel_size=kernel_size,\n",
        "                                         strides=strides, \n",
        "                                         padding='same',\n",
        "                                         use_bias=False,\n",
        "                                         name='upscale_block_' + name + '_conv2d', \n",
        "                                         activation=None)(tensor)\n",
        "    _x = tf.keras.layers.BatchNormalization(name='upscale_block_' + name + '_bn')(_x)\n",
        "    _x = tf.keras.layers.ReLU(name='upscale_block_' + name + '_relu')(_x)\n",
        "    return _x\n",
        "\n",
        "def build_generator():\n",
        "    _input = tf.keras.Input(shape=[256, 256, 1], name='image_input')\n",
        "    x = downscale_conv2D(_input, 64, strides=1, name='0')\n",
        "    features = [x]\n",
        "    for i, n_filters in enumerate([64, 128, 256, 512, 512, 512, 512]):\n",
        "        x = downscale_conv2D(x, n_filters, name=str(i+1))\n",
        "        features.append(x)\n",
        "\n",
        "    for i, n_filters in enumerate([512, 512, 512, 256, 128, 64, 64]):\n",
        "        x = upscale_deconv2d(x, n_filters, name=str(i+1))\n",
        "        x = tf.keras.layers.Concatenate()([features[-(i+2)], x])\n",
        "    _output = tf.keras.layers.Conv2D(filters=3, \n",
        "                                     kernel_size=1, \n",
        "                                     strides=1, \n",
        "                                     padding='same',\n",
        "                                     name='output_conv2d', \n",
        "                                     activation='tanh')(x)\n",
        "    return tf.keras.Model(inputs=[_input], outputs=[_output], name='Generator')\n",
        "\n",
        "def build_discriminator():\n",
        "    _input = tf.keras.Input(shape=[256, 256,  4])\n",
        "    x = downscale_conv2D(_input, 64, strides=2, name='0', use_bn=False)\n",
        "    x = downscale_conv2D(x, 128, strides=2, name='1')\n",
        "    x = downscale_conv2D(x, 256, strides=2, name='2')\n",
        "    x = downscale_conv2D(x, 512, strides=1, name='3')\n",
        "    _output = tf.keras.layers.Conv2D(filters=1,\n",
        "                                     kernel_size=1, \n",
        "                                     strides=1, \n",
        "                                     padding='same', \n",
        "                                     name='output_conv2d', \n",
        "                                     activation=None)(x)\n",
        "    return tf.keras.Model(inputs=[_input], outputs=[_output], name='Discriminator')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKFZVTEuXu3e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Colorizer:\n",
        "    def __init__(self, config):\n",
        "        super(Colorizer, self).__init__()\n",
        "        self.distribute_strategy = config['distribute_strategy']\n",
        "        self.epochs = config['epochs']\n",
        "        self.batch_size = config['batch_size']\n",
        "        self.d_lr = config['d_lr']\n",
        "        self.g_lr = config['g_lr']\n",
        "        self.model_dir = config['model_dir']\n",
        "        self.tensorboard_log_dir = config['tensorboard_log_dir']\n",
        "        self.checkpoint_prefix = config['checkpoint_prefix']\n",
        "        self.restore_parameters = config['restore_parameters']\n",
        "        self.build_dataset()\n",
        "        self.create_optimizers()\n",
        "        self.initialize_loss_objects()\n",
        "        self.initialize_metrics()\n",
        "        self.build_models()\n",
        "        self.initialize_checkpoint_manager()\n",
        "        self.create_summary_writer()\n",
        "        \n",
        "    def build_models(self):        \n",
        "        with self.distribute_strategy.scope():\n",
        "            self.generator = build_generator()\n",
        "            self.discriminator = build_discriminator()\n",
        "            \n",
        "    @staticmethod\n",
        "    def preprocess_input(sample):\n",
        "        image = sample['image']\n",
        "        def _preprocess_input(image):\n",
        "            image_n = image.numpy()\n",
        "            image_gray = rgb2gray(image_n)\n",
        "            image_lab = rgb2lab(image_n)\n",
        "            return image_gray, image_lab\n",
        "        return tf.py_function(_preprocess_input, [image], [tf.float32, tf.float32])\n",
        "\n",
        "    def build_dataset(self):\n",
        "        self.dataset = tfds.load(name='places365_small', as_supervised=False)\n",
        "        self.dataset = self.dataset.map(Colorizer.preprocess_input,\n",
        "                                        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "        self.datset = self.dataset.batch(self.batch_size, drop_remainder=True)\n",
        "        self.dataset = self.dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "        \n",
        "    def create_optimizers(self):\n",
        "        with self.distribute_strategy.scope():\n",
        "            self.d_optimizer = tf.keras.optimizers.Adam(learning_rate=self.d_lr)\n",
        "            self.g_optimizer = tf.keras.optimizers.Adam(learning_rate=self.g_lr)  \n",
        "    \n",
        "    def initialize_loss_objects(self):\n",
        "        with self.distribute_strategy.scope():\n",
        "            self.bce_smooth = tf.keras.losses.BinaryCrossentropy(from_logits=True, label_smoothing=0.1, reduction=tf.keras.losses.Reduction.NONE)\n",
        "            self.bce = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)     \n",
        "    \n",
        "    def initialize_metrics(self):\n",
        "        with self.distribute_strategy.scope():\n",
        "            self.generator_loss = tf.keras.metrics.Mean(name='generator_loss')\n",
        "            self.discriminator_loss_real = tf.keras.metrics.Mean(name='discriminator_loss_real')   \n",
        "            self.discriminator_loss_fake = tf.keras.metrics.Mean(name='discriminator_loss_fake')\n",
        "            self.psnr = tf.keras.metrics.Mean(name='PSNR')\n",
        "            \n",
        "    def create_checkpoint_manager(self):\n",
        "        with self.distribute_strategy.scope():\n",
        "            self.checkpoint = tf.train.Checkpoint(generator=self.generator, \n",
        "                                                  discriminator=self.discriminator, \n",
        "                                                  g_optimizer=self.g_optimizer,\n",
        "                                                  d_optimizer=self.d_optimizer)\n",
        "            if self.restore_parameters:\n",
        "                latest_checkpoint = tf.train.latest_checkpoint(self.model_dir)\n",
        "                self.status = self.checkpoint.restore(latest_checkpoint)\n",
        "                        \n",
        "    def create_summary_writer(self):\n",
        "        self.summary_writer = tf.summary.create_file_writer(logdir=self.tensorboard_log_dir)\n",
        "            \n",
        "\n",
        "    def loss_G(self, fake_logits):\n",
        "        bce_loss = tf.reduce_mean(self.bce(tf.ones_like(fake_logits), fake_logits), axis=[1, 2])\n",
        "        l1_loss = tf.reduce_mean(tf.abs(real - fake), axis=[1, 2])\n",
        "        return tf.nn.compute_average_loss(bce_loss + l1_loss, global_batch_size=self.batch_size)\n",
        "\n",
        "    def loss_D_real(self, real_logits):\n",
        "        real_loss = tf.reduce_mean(self.bce_smooth(tf.ones_like(real_logits), real_logits), axis=[1, 2])\n",
        "        return tf.nn.compute_average_loss(real_loss, global_batch_size=self.batch_size)\n",
        "\n",
        "    def loss_D_fake(self, fake_logits):\n",
        "        fake_loss = tf.reduce_mean(bce_loss_fn(tf.zeros_like(fake_logits), fake_logits), axis=[1, 2])\n",
        "        return tf.nn.compute_average_loss(fake_loss, global_batch_size=self.batch_size)\n",
        "    \n",
        "    def compute_psnr(self, real, fake):\n",
        "        psnr = tf.image.psnr(real, fake, max_val=1.0)\n",
        "        return tf.reduce_sum(psnr, axis=0) / self.batch_size\n",
        "    \n",
        "    def write_summaries(self, metrics):\n",
        "        d_real_loss, d_fake_loss, g_loss, psnr = metrics\n",
        "        with self.summary_writer.as_default():\n",
        "            tf.summary.scalar('discriminator_loss_real', d_real_loss, step=self.iterations)\n",
        "            tf.summary.scalar('discriminator_loss_fake', d_fake_loss, step=self.iterations)\n",
        "            tf.summary.scalar('generator_loss', g_loss, step=self.iterations)\n",
        "            tf.summary.scalar('PSNR', psnr, step=self.iterations)\n",
        "            \n",
        "    def write_checkpoint(self):\n",
        "        with self.distribute_strategy.scope():\n",
        "            self.checkpoint.save(os.path.join(self.model_dir, self.checkpoint_prefix))            \n",
        "    \n",
        "    def update_metrics(self, metrics):\n",
        "        d_real_loss, d_fake_loss, g_loss, psnr = metrics\n",
        "        self.generator_loss.update_state(g_loss)\n",
        "        self.discriminator_loss_real.update_state(d_real_loss)\n",
        "        self.discriminator_loss_fake.update_state(d_fake_loss)\n",
        "        self.psnr.update_state(psnr)\n",
        "    \n",
        "    def reset_metrics(self):\n",
        "        self.generator_loss.reset_states()\n",
        "        self.discriminator_loss_real.reset_states()\n",
        "        self.discriminator_loss_fake.reset_states()\n",
        "        self.psnr.reset_states()\n",
        "        \n",
        "\n",
        "    def train(self):\n",
        "        if self.restore_parameters:\n",
        "            self.restore_status.assert_consumed()\n",
        "            \n",
        "        @tf.function\n",
        "        def train_step(grayscale_image, lab_image):\n",
        "            real_input = tf.concat([grayscale_image, lab_image], axis=-1)\n",
        "            with tf.GradientTape() as r_tape:\n",
        "                real_logits = self.discriminator(real_input, training=True)\n",
        "                d_real_loss = self.loss_D_real(real_logits)\n",
        "            d_r_gradients = r_tape.gradient(d_real_loss, self.discriminator.trainable_variables)\n",
        "            self.d_optimizer.apply_gradients(zip(d_r_gradients, self.discriminator.trainable_variables))\n",
        "\n",
        "            with tf.GradientTape() as g_tape, tf.GradientTape() as d_tape:\n",
        "                fake_image = self.generator(grayscale_image, training=True)\n",
        "                fake_input = tf.concat([grayscale_image, fake_image], axis=-1)\n",
        "                fake_logits = self.discriminator(real_input, training=True)\n",
        "\n",
        "                d_fake_loss = self.loss_D_fake(fake_logits)\n",
        "                g_loss = self.loss_G(fake_logits)\n",
        "            d_f_gradients = d_tape.gradient(d_fake_loss, self.discriminator.trainable_variables)\n",
        "            g_gradients = g_tape.gradient(g_loss, self.generator.trainable_variables)\n",
        "            self.d_optimizer.apply_gradients(zip(d_f_gradients, self.discriminator.trainable_variables))\n",
        "            self.g_optimizer.apply_gradients(zip(g_gradients, self.generator.trainable_variables))\n",
        "            psnr = self.compute_psnr(lab_image, fake_image)\n",
        "            return tf.constant([d_real_loss, d_fake_loss, g_loss, psnr])\n",
        "    \n",
        "        @tf.function\n",
        "        def distributed_train_step(grayscale_image, lab_image):\n",
        "            per_replica_loss = self.strategy.experimental_run_v2(fn=train_step, args=(grayscale_image, lab_image))\n",
        "            total_loss = self.strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_loss, axis=0)\n",
        "            return total_loss\n",
        "        \n",
        "        for ep in self.epochs:\n",
        "            self.iterations = 0\n",
        "            for grayscale_image, lab_image in self.dataset:\n",
        "                metrics = distributed_train_step(grayscale_image, lab_image)\n",
        "                self.update_metrics(metrics)\n",
        "                self.iterations += 1\n",
        "            self.write_summaries(metrics)\n",
        "            self.reset_metrics()\n",
        "            self.write_checkpoint()\n",
        "                    \n",
        "            \n",
        " '''\n",
        " To Do \n",
        "1. Build Dataset\n",
        "2. Preprocessing function, write color conversion in pure tensorflow\n",
        "3. Inference Loop\n",
        "4. TPU Compability"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTpRx2qxXu3g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = tfds.load(name='places365_small', as_supervised=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdtbhONbXu3j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}