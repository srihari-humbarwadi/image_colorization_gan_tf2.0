{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 2.0.0\n",
      "TensorFlow Datasets 1.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from skimage.color import rgb2lab, lab2rgb, rgb2gray\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "print('TensorFlow', tf.__version__)\n",
    "print('TensorFlow Datasets', tfds.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downscale_conv2D(tensor, n_filters, kernel_size=4, strides=2, name=None, use_bn=True):\n",
    "    _x = tf.keras.layers.Conv2D(filters=n_filters,\n",
    "                                kernel_size=kernel_size,\n",
    "                                strides=strides, \n",
    "                                padding='same',\n",
    "                                use_bias=False,\n",
    "                                name='downscale_block_' + name + '_conv2d', \n",
    "                                activation=None)(tensor)\n",
    "    if use_bn:\n",
    "        _x = tf.keras.layers.BatchNormalization(name='downscale_block_' + name + '_bn')(_x)\n",
    "    _x = tf.keras.layers.LeakyReLU(alpha=0.2, name='downscale_block_' + name + '_lrelu')(_x)\n",
    "    return _x\n",
    "\n",
    "def upscale_deconv2d(tensor, n_filters, kernel_size=4, strides=2, name=None):\n",
    "    _x = tf.keras.layers.Conv2DTranspose(filters=n_filters,\n",
    "                                         kernel_size=kernel_size,\n",
    "                                         strides=strides, \n",
    "                                         padding='same',\n",
    "                                         use_bias=False,\n",
    "                                         name='upscale_block_' + name + '_conv2d', \n",
    "                                         activation=None)(tensor)\n",
    "    _x = tf.keras.layers.BatchNormalization(name='upscale_block_' + name + '_bn')(_x)\n",
    "    _x = tf.keras.layers.ReLU(name='upscale_block_' + name + '_relu')(_x)\n",
    "    return _x\n",
    "\n",
    "def build_generator():\n",
    "    _input = tf.keras.Input(shape=[256, 256, 1], name='image_input')\n",
    "    x = downscale_conv2D(_input, 64, strides=1, name='0')\n",
    "    features = [x]\n",
    "    for i, n_filters in enumerate([64, 128, 256, 512, 512, 512, 512]):\n",
    "        x = downscale_conv2D(x, n_filters, name=str(i+1))\n",
    "        features.append(x)\n",
    "\n",
    "    for i, n_filters in enumerate([512, 512, 512, 256, 128, 64, 64]):\n",
    "        x = upscale_deconv2d(x, n_filters, name=str(i+1))\n",
    "        x = tf.keras.layers.Concatenate()([features[-(i+2)], x])\n",
    "    _output = tf.keras.layers.Conv2D(filters=3, \n",
    "                                     kernel_size=1, \n",
    "                                     strides=1, \n",
    "                                     padding='same',\n",
    "                                     name='output_conv2d', \n",
    "                                     activation='tanh')(x)\n",
    "    return tf.keras.Model(inputs=[_input], outputs=[_output], name='Generator')\n",
    "\n",
    "def build_discriminator():\n",
    "    _input = tf.keras.Input(shape=[256, 256,  4])\n",
    "    x = downscale_conv2D(_input, 64, strides=2, name='0', use_bn=False)\n",
    "    x = downscale_conv2D(x, 128, strides=2, name='1')\n",
    "    x = downscale_conv2D(x, 256, strides=2, name='2')\n",
    "    x = downscale_conv2D(x, 512, strides=1, name='3')\n",
    "    _output = tf.keras.layers.Conv2D(filters=1,\n",
    "                                     kernel_size=1, \n",
    "                                     strides=1, \n",
    "                                     padding='same', \n",
    "                                     name='output_conv2d', \n",
    "                                     activation=None)(x)\n",
    "    return tf.keras.Model(inputs=[_input], outputs=[_output], name='Discriminator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Colorizer:\n",
    "    def __init__(self, config):\n",
    "        super(Colorizer, self).__init__()\n",
    "        self.distribute_strategy = config['distribute_strategy']\n",
    "#         self.build_models()\n",
    "        self.epochs = config['epochs']\n",
    "        self.batch_size = config['batch_size']\n",
    "        self.build_dataset()\n",
    "        self.d_optimizer = tf.keras.optimizers.Adam(learning_rate=config['d_lr'])\n",
    "        self.g_optimizer = tf.keras.optimizers.Adam(learning_rate=config['g_lr'])        \n",
    "        \n",
    "        \n",
    "    def build_models(self):        \n",
    "        with self.distribute_strategy.scope():\n",
    "            self.generator = build_generator()\n",
    "            self.discriminator = build_discriminator()\n",
    "            \n",
    "    @staticmethod\n",
    "    def preprocess_input(image):\n",
    "        def _preprocess_input(image):\n",
    "            image_n = image.numpy()\n",
    "            image_gray = rgb2gray(image_n)\n",
    "            image_lab = rgb2lab(image_n)\n",
    "            return image_gray, image_lab\n",
    "        return tf.py_function(_preprocess_input, [image], [tf.float32, tf.float32])\n",
    "\n",
    "    def build_dataset(self):\n",
    "#         dataset = tfds.load(name='places365_small', as_supervised=False)\n",
    "        images = tf.random.uniform(shape=[1000, 256, 256, 3], maxval=255)\n",
    "        self.dataset = tf.data.Dataset.from_tensor_slices(images)\n",
    "        self.dataset = self.dataset.map(Colorizer.preprocess_input,\n",
    "                                        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "        self.datset = self.dataset.batch(self.batch_size, drop_remainder=True)\n",
    "        self.dataset = self.dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "        return self.dataset\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        def distributed_train_step(batch):\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "846de2b76f02479387a0802dbb63761c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'distribute_strategy': tf.distribute.OneDeviceStrategy(device='/cpu:0'),\n",
    "    'epochs':20,\n",
    "    'batch_size':16,\n",
    "    'd_lr':1e-4,\n",
    "    'g_lr':1e-4\n",
    "}\n",
    "\n",
    "colorizer = Colorizer(config)\n",
    "colorizer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.distribute.OneDeviceStrategy.experimental_run_v2()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
